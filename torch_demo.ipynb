{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/disk/p/jkcm/Code')\n",
    "from tools.LoopTimer import LoopTimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "DATA_PATH = r'/home/disk/eos4/jkcm/Data/MNISTData'\n",
    "MODEL_STORE_PATH = r'/home/disk/p/jkcm/Projects/MNISTmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f5da2f842d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise RuntimeError('Dataset not found.' +\n\u001b[0;32m---> 72\u001b[0;31m                                ' You can use download=True to download it')\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "# # transforms to apply to the data\n",
    "# trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# # MNIST dataset\n",
    "# train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=False)\n",
    "# test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.33%      ETA 17:27:17      time left: 13.3 minutes       Epoch [1/5], Step [100/600], Loss: 0.2374, Accuracy: 91.00%\n",
      "6.67%      ETA 17:27:19      time left: 12.9 minutes       Epoch [1/5], Step [200/600], Loss: 0.0907, Accuracy: 97.00%\n",
      "10.00%      ETA 17:27:21      time left: 12.5 minutes       Epoch [1/5], Step [300/600], Loss: 0.2435, Accuracy: 92.00%\n",
      "13.33%      ETA 17:27:20      time left: 12.0 minutes       Epoch [1/5], Step [400/600], Loss: 0.1354, Accuracy: 97.00%\n",
      "16.67%      ETA 17:27:21      time left: 11.5 minutes       Epoch [1/5], Step [500/600], Loss: 0.0660, Accuracy: 99.00%\n",
      "20.00%      ETA 17:27:21      time left: 11.1 minutes       Epoch [1/5], Step [600/600], Loss: 0.0867, Accuracy: 97.00%\n",
      "23.33%      ETA 17:27:22      time left: 10.6 minutes       Epoch [2/5], Step [100/600], Loss: 0.0575, Accuracy: 98.00%\n",
      "26.67%      ETA 17:27:25      time left: 10.2 minutes       Epoch [2/5], Step [200/600], Loss: 0.1328, Accuracy: 96.00%\n",
      "30.00%      ETA 17:27:28      time left: 9.8 minutes       Epoch [2/5], Step [300/600], Loss: 0.0287, Accuracy: 99.00%\n",
      "33.33%      ETA 17:27:29      time left: 9.3 minutes       Epoch [2/5], Step [400/600], Loss: 0.1441, Accuracy: 98.00%\n",
      "36.67%      ETA 17:27:29      time left: 8.8 minutes       Epoch [2/5], Step [500/600], Loss: 0.0900, Accuracy: 98.00%\n",
      "40.00%      ETA 17:27:29      time left: 8.4 minutes       Epoch [2/5], Step [600/600], Loss: 0.0910, Accuracy: 99.00%\n",
      "43.33%      ETA 17:27:30      time left: 7.9 minutes       Epoch [3/5], Step [100/600], Loss: 0.0182, Accuracy: 99.00%\n",
      "46.67%      ETA 17:27:31      time left: 7.5 minutes       Epoch [3/5], Step [200/600], Loss: 0.0476, Accuracy: 99.00%\n",
      "50.00%      ETA 17:27:31      time left: 7.0 minutes       Epoch [3/5], Step [300/600], Loss: 0.0292, Accuracy: 99.00%\n",
      "53.33%      ETA 17:27:31      time left: 6.5 minutes       Epoch [3/5], Step [400/600], Loss: 0.0367, Accuracy: 99.00%\n",
      "56.67%      ETA 17:27:32      time left: 6.1 minutes       Epoch [3/5], Step [500/600], Loss: 0.0269, Accuracy: 99.00%\n",
      "60.00%      ETA 17:27:32      time left: 5.6 minutes       Epoch [3/5], Step [600/600], Loss: 0.0384, Accuracy: 99.00%\n",
      "63.33%      ETA 17:27:32      time left: 5.1 minutes       Epoch [4/5], Step [100/600], Loss: 0.0523, Accuracy: 98.00%\n",
      "66.67%      ETA 17:27:32      time left: 4.7 minutes       Epoch [4/5], Step [200/600], Loss: 0.0776, Accuracy: 96.00%\n",
      "70.00%      ETA 17:27:32      time left: 4.2 minutes       Epoch [4/5], Step [300/600], Loss: 0.0276, Accuracy: 98.00%\n",
      "73.33%      ETA 17:27:32      time left: 3.7 minutes       Epoch [4/5], Step [400/600], Loss: 0.0350, Accuracy: 98.00%\n",
      "76.67%      ETA 17:27:33      time left: 3.3 minutes       Epoch [4/5], Step [500/600], Loss: 0.1195, Accuracy: 96.00%\n",
      "80.00%      ETA 17:27:33      time left: 168 seconds       Epoch [4/5], Step [600/600], Loss: 0.0109, Accuracy: 100.00%\n",
      "83.33%      ETA 17:27:33      time left: 140 seconds       Epoch [5/5], Step [100/600], Loss: 0.0253, Accuracy: 99.00%\n",
      "86.67%      ETA 17:27:33      time left: 112 seconds       Epoch [5/5], Step [200/600], Loss: 0.0505, Accuracy: 99.00%\n",
      "90.00%      ETA 17:27:33      time left: 84 seconds       Epoch [5/5], Step [300/600], Loss: 0.1173, Accuracy: 97.00%\n",
      "93.33%      ETA 17:27:33      time left: 56 seconds       Epoch [5/5], Step [400/600], Loss: 0.1258, Accuracy: 97.00%\n",
      "96.67%      ETA 17:27:33      time left: 28 seconds       Epoch [5/5], Step [500/600], Loss: 0.0214, Accuracy: 99.00%\n",
      "100.00%      ETA 17:27:33      time left: 0 seconds       Epoch [5/5], Step [600/600], Loss: 0.0244, Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "lt = LoopTimer(num_epochs*len(train_loader))\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        lt.update()\n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 96.17 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/p/jkcm/anaconda3/lib/python3.5/site-packages/bokeh/models/sources.py:110: BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('x', 103), ('y', 102)\n",
      "  \"Current lengths: %s\" % \", \".join(sorted(str((k, len(v))) for k, v in data.items())), BokehUserWarning))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n",
    "\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
